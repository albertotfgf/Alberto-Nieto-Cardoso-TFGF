{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f6302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============\n",
    "# CARGA DE LIBRERIAS\n",
    "\n",
    "from astroquery.utils.tap.core import TapPlus\n",
    "from astroquery.ipac.ned import Ned\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from SciServer import Authentication\n",
    "from SciServer import CasJobs as cj\n",
    "\n",
    "# Iniciar sesion\n",
    "username = \"username\"\n",
    "password = \"password\"\n",
    "Authentication.login(UserName = username, Password = password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fad9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# BUCLE PARA OBTENER LOS .fits DE LA SDSS A PARTIR DE UN CIERTO REDSHIFT MINIMO ACTUALIZADO EN CADA ITERACION\n",
    "\n",
    "# Modificar redshift minimo\n",
    "# ultimo_redshift = 0\n",
    "# with open('extra/redshift_init.pickle', 'wb') as handle:\n",
    "#     pickle.dump(ultimo_redshift, handle)\n",
    "\n",
    "# Parametros iniciales\n",
    "batch_size_inicial = 50000       # tamaño inicial del lote\n",
    "min_batch_size = 1000            # tamaño minimo de lote permitido\n",
    "max_retries = 10                 # maximo numero de reintentos por lote\n",
    "offset = 0                       # parametro de parada\n",
    "all_results = []\n",
    "batch_results = []\n",
    "\n",
    "while offset < 1000000:\n",
    "\n",
    "    with open('extra/redshift_init.pickle', 'rb') as handle:\n",
    "        redshift_init = pickle.load(handle)\n",
    "\n",
    "    redshift_init = float(str(redshift_init)[:8]) # Solo se toman los primeros 7 digitos, si no se excede el límite de la consulta\n",
    "    current_batch_size = batch_size_inicial\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    # Intentar ejecutar el query con el tamaño de lote actual. Si hay error, se reduce el lote y se reintenta.\n",
    "    while not success and retries < max_retries:\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT\n",
    "            p.objid,\n",
    "            s.specObjID,\n",
    "            dbo.fGetUrlFitsSpectrum(s.specObjID) AS fits_url,\n",
    "            s.z AS redshift\n",
    "        FROM \n",
    "            PhotoObj AS p\n",
    "        JOIN \n",
    "            SpecObj AS s ON s.bestobjid = p.objid\n",
    "        WHERE \n",
    "            s.z BETWEEN {redshift_init} AND 9\n",
    "            AND s.zWarning = 0\n",
    "            AND (s.class = 'QSO' OR s.class = 'GALAXY')\n",
    "        ORDER BY s.z\n",
    "        OFFSET 0 ROWS\n",
    "        FETCH NEXT {current_batch_size} ROWS ONLY\n",
    "        \"\"\"\n",
    "        try:\n",
    "            batch_results = pd.DataFrame(cj.executeQuery(sql_query, context=\"DR18\"))\n",
    "            success = True\n",
    "            all_results.append(batch_results)\n",
    "            offset += len(batch_results)\n",
    "\n",
    "            with open('extra/all_results.pickle', 'wb') as handle:\n",
    "                pickle.dump(all_results, handle)\n",
    "            print(f\"Recuperados {offset} registros hasta ahora.\")\n",
    "\n",
    "            ultimo_redshift = batch_results[\"redshift\"].iloc[-1]\n",
    "\n",
    "            ultimo_redshift = float(str(ultimo_redshift)[:8])\n",
    "            print(f\"Redshift inicial: {redshift_init}\")\n",
    "            print(f\"Redshift último: {ultimo_redshift}\")\n",
    "            if ultimo_redshift == redshift_init:    # Si el redshift no cambio, se incrementa un poco para evitar loops infinitos\n",
    "                ultimo_redshift = ultimo_redshift + 0.000002\n",
    "                print(f\"Se incrementó el redshift a {ultimo_redshift}\")\n",
    "\n",
    "            with open('extra/redshift_init.pickle', 'wb') as handle:\n",
    "                pickle.dump(ultimo_redshift, handle)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en la consulta con batch_size = {current_batch_size}: {e}\")\n",
    "            # Reducir el tamaño del lote, pero sin bajar de min_batch_size\n",
    "            current_batch_size = max(min_batch_size, current_batch_size // 2)\n",
    "            retries += 1\n",
    "            time.sleep(2)  # Esperar un poco antes de reintentar\n",
    "\n",
    "    if len(batch_results) == 0:\n",
    "        print(\"No hay más registros para recuperar.\")\n",
    "        break\n",
    "\n",
    "    if retries == max_retries:\n",
    "        print(\"No se pudo recuperar el batch tras varios reintentos. Terminando.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extra/all_results.pickle', 'rb') as handle:\n",
    "    all_results = pickle.load(handle)\n",
    "\n",
    "# Combinar todos los resultados\n",
    "combined_results = pd.concat(all_results, ignore_index=True)\n",
    "print(f\"Total de registros recuperados: {len(combined_results)}\")\n",
    "\n",
    "# Extraer las URLs de los archivos FITS\n",
    "fits_urls = combined_results[\"fits_url\"].tolist()\n",
    "print(f\"Se han obtenido {len(fits_urls)} URLs de FITS.\")\n",
    "\n",
    "ultimo_redshift = all_results[len(all_results)-2][\"redshift\"].iloc[0]\n",
    "print(f\"Último redshift recuperado: {ultimo_redshift:.7f}\")\n",
    "\n",
    "with open('extra/redshift_init.pickle', 'wb') as handle:\n",
    "    pickle.dump(ultimo_redshift, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# DSECARGA EN PARALELO DE LOS .fits OBTENIDOS\n",
    "\n",
    "# Directorio de descarga\n",
    "output_dir = r\"spectrums\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "archivos_descargados = []\n",
    "\n",
    "def download_file(url):\n",
    "    filename = os.path.basename(url)\n",
    "    local_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Saltar si ya se descargo este archivo\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"Saltando {filename} (ya descargado).\")\n",
    "        return filename, None\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Descargado: {filename}\")\n",
    "        return filename, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar {url}: {e}\")\n",
    "        return filename, e\n",
    "\n",
    "# Numero de descargas en paralelo\n",
    "paralelo = 100\n",
    "i=0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=paralelo) as executor:\n",
    "    future_to_url = {executor.submit(download_file, url): url for url in fits_urls}\n",
    "    \n",
    "    for future in as_completed(future_to_url):\n",
    "        i+=1\n",
    "        print(f\"Descargando archivo {i} de {len(fits_urls)}\")\n",
    "        filename, error = future.result()\n",
    "        if error is None:\n",
    "            archivos_descargados.append(filename)\n",
    "\n",
    "print(\"Descarga finalizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83830b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# CODIGO COMPACTO PARA LA DESCARGA DE ESPECTROS DE LA NED\n",
    "\n",
    "# Funciones y configuración inicial\n",
    "\n",
    "def sanitize(name):\n",
    "    \"\"\"Reemplaza caracteres inválidos para Windows por '_'.\"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9_-]', '_', name)\n",
    "\n",
    "output_dir    = \"spectrums NASA\"\n",
    "redshift_path = 'extra/redshift_init_ned.pickle'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Modificar redshift minimo\n",
    "redshift_init = 0\n",
    "with open(redshift_path, 'wb') as handle:\n",
    "    pickle.dump(redshift_init, handle)\n",
    "\n",
    "batch_size_in = 10000      # lote inicial ADQL\n",
    "max_retries   = 5          # reintentos por lote\n",
    "\n",
    "all_meta = []              # acumulador de metadatos\n",
    "\n",
    "# Paginacion por redshift para metadatos\n",
    "while True:\n",
    "\n",
    "    with open(redshift_path, 'rb') as f:\n",
    "        redshift_init = float(str(pickle.load(f))[:8])\n",
    "        print(f\"Redshift actual: {redshift_init}\")\n",
    "        print(f\"Redshift actual: {redshift_init:.7f}\")\n",
    "\n",
    "    current_batch = batch_size_in\n",
    "    retries       = 0\n",
    "    success       = False\n",
    "\n",
    "    # Intentos de consulta con reduccion de lote\n",
    "    while not success and retries < max_retries:\n",
    "        adql = f\"\"\"\n",
    "        SELECT TOP {current_batch}\n",
    "            prefname, z\n",
    "        FROM NEDTAP.objdir\n",
    "        WHERE\n",
    "            z >= {redshift_init:.7f}\n",
    "            AND z < 9.0\n",
    "            AND n_spectra > 0\n",
    "        ORDER BY z\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tap     = TapPlus(url=\"https://ned.ipac.caltech.edu/tap\")\n",
    "            batch   = tap.launch_job(adql).get_results().to_pandas()\n",
    "            success = True\n",
    "\n",
    "            if batch.empty:\n",
    "                break\n",
    "\n",
    "            all_meta.append(batch)\n",
    "\n",
    "            # Actualizar z_init  \n",
    "            last_z = float(str(batch['z'].iloc[-1])[:8])\n",
    "            if last_z == redshift_init:\n",
    "                last_z += 0.000002\n",
    "            with open(redshift_path, 'wb') as f:\n",
    "                pickle.dump(last_z, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(e)\n",
    "            time.sleep(2)\n",
    "\n",
    "    if not success or batch.empty:\n",
    "        break\n",
    "\n",
    "meta_df = pd.concat(all_meta, ignore_index=True)\n",
    "\n",
    "# Descarga en paralelo con manejo de existentes\n",
    "def download_spectra(prefname, z):\n",
    "    base = sanitize(prefname)\n",
    "    saved = []\n",
    "    try:\n",
    "        spectra = Ned.get_spectra(prefname, show_progress=False)\n",
    "    except Exception:\n",
    "        return saved\n",
    "\n",
    "    for idx, hdulist in enumerate(spectra, start=1):\n",
    "        filename = f\"{base}_{idx}_z{z:.5f}.fits\"\n",
    "        path     = os.path.join(output_dir, filename)\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Saltando (ya existe): {filename}\")\n",
    "        else:\n",
    "            hdulist.writeto(path, overwrite=True)\n",
    "            print(f\"Guardado: {filename}\")\n",
    "        saved.append(path)\n",
    "    return saved\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    futures = {\n",
    "        executor.submit(download_spectra, row['prefname'], row['z']): ix\n",
    "        for ix, row in meta_df.iterrows()\n",
    "    }\n",
    "    for future in as_completed(futures):\n",
    "        pass\n",
    "\n",
    "print(f\"Redshift final: {redshift_init}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFGF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
