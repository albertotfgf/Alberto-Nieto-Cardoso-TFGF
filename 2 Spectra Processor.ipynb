{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bb617a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21593,
     "status": "ok",
     "timestamp": 1740659506181,
     "user": {
      "displayName": "Alberto TFG",
      "userId": "08861594150133217172"
     },
     "user_tz": -60
    },
    "id": "14bb617a",
    "outputId": "9441a28a-8e5c-4469-e98c-e02709b1cb34"
   },
   "outputs": [],
   "source": [
    "# =============\n",
    "# CARGA DE LIBRERIAS\n",
    "\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# INTERPOLACION DE LOS ESPECTROS Y GUARDADO EN FORMATO PICKLE\n",
    "\n",
    "folder_path = r'spectrums'\n",
    "batch_size = 10000  # Numero de archivos a procesar por lote\n",
    "index_file = \"extra/batch_index.txt\"\n",
    "\n",
    "# Verificar que el archivo batch_index.txt exista\n",
    "if not os.path.exists(index_file):\n",
    "    print(f\"Error: El archivo {index_file} no existe. Deteniendo la ejecución.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Intentar leer y convertir el contenido a un numero entero\n",
    "with open(index_file, \"r\") as f:\n",
    "    content = f.read().strip()\n",
    "    try:\n",
    "        batch_index = int(content)\n",
    "    except ValueError:\n",
    "        print(f\"Error: El archivo {index_file} no contiene un número entero válido. Deteniendo la ejecución.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def expand_points(wavelength, flux, target_count=5000):\n",
    "    # Convertir a listas para facilitar las inserciones\n",
    "    wl = list(wavelength)\n",
    "    fl = list(flux)\n",
    "    \n",
    "    # Calcular las diferencias absolutas entre puntos consecutivos\n",
    "    diffs = [abs(fl[i+1] - fl[i]) for i in range(len(fl)-1)]\n",
    "    # Obtener los indices ordenados de mayor a menor diferencia\n",
    "    sorted_indices = sorted(range(len(diffs)), key=lambda i: diffs[i], reverse=True)\n",
    "    \n",
    "    # Insertar nuevos puntos utilizando los indices ordenados\n",
    "    while len(wl) < target_count:\n",
    "        # Se recorre la lista de indices en orden descendente para evitar problemas con el reordenamiento\n",
    "        for idx in sorted_indices:\n",
    "            if len(wl) >= target_count:\n",
    "                break\n",
    "            # Calcular la interpolacion lineal entre el punto idx y el siguiente\n",
    "            new_wl = (wl[idx] + wl[idx+1]) / 2\n",
    "            new_fl = (fl[idx] + fl[idx+1]) / 2\n",
    "            # Insertar el nuevo punto en la posicion correspondiente\n",
    "            wl.insert(idx+1, new_wl)\n",
    "            fl.insert(idx+1, new_fl)\n",
    "    \n",
    "    return np.array(wl), np.array(fl)\n",
    "\n",
    "# Obtener la lista inicial de archivos FITS\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.fits')]\n",
    "\n",
    "while files:\n",
    "    # Seleccionar aleatoriamente el lote actual (si quedan menos de batch_size, se toman todos)\n",
    "    if len(files) < batch_size:\n",
    "        current_batch = files.copy()\n",
    "    else:\n",
    "        current_batch = random.sample(files, batch_size)\n",
    "    \n",
    "    # Diccionario de almacenamiento para el lote actual\n",
    "    spectra_data = {}\n",
    "    i = 0\n",
    "\n",
    "    for filename in current_batch:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with fits.open(file_path) as hdul:\n",
    "                # Verifica que el archivo tenga las extensiones esperadas\n",
    "                if len(hdul) > 2:\n",
    "                    flux_data = hdul[1].data[\"flux\"]            # Datos de flujo\n",
    "                    loglam_data = hdul[1].data[\"loglam\"]        # Datos de log(lambda)\n",
    "                    wavelength_data = 10 ** loglam_data         # Convertir log(lambda) a longitud de onda\n",
    "                    redshift = hdul[2].data[\"Z\"][0]             # Extraer redshift\n",
    "                    \n",
    "                    # Aplicar la interpolacion para obtener 5000 puntos\n",
    "                    interp_wavelength, interp_flux = expand_points(wavelength_data, flux_data, target_count=5000)\n",
    "                    \n",
    "                    # Guardar los datos\n",
    "                    spectra_data[filename] = {\n",
    "                        \"wavelength\": interp_wavelength,\n",
    "                        \"flux\": interp_flux,\n",
    "                        \"redshift\": redshift\n",
    "                    }\n",
    "                    \n",
    "                    i += 1\n",
    "                    if i % 1000 == 0:\n",
    "                        print(f\"Procesado {filename} ({i})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {filename}: {e}\")\n",
    "    \n",
    "    print(f\"Se procesaron {len(spectra_data)} archivos FITS en el lote {batch_index}.\")\n",
    "    \n",
    "    # Guardar el diccionario en un archivo pickle\n",
    "    output_file = f'data/spectra_data_complete{batch_index}.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(spectra_data, f)\n",
    "    print(f\"Datos guardados en {output_file}\")\n",
    "    \n",
    "    # Guardar la lista de archivos procesados en este lote\n",
    "    batch_list_file = f'extra/batch_files_list{batch_index}.pkl'\n",
    "    with open(batch_list_file, 'wb') as f:\n",
    "        pickle.dump(current_batch, f)\n",
    "    print(f\"Lista de archivos del lote {batch_index} guardada en {batch_list_file}\")\n",
    "    \n",
    "    # Eliminar los nuevos procesados de files\n",
    "    files = [f for f in files if f not in current_batch]\n",
    "    \n",
    "    # Actualizar el archivo batch_index.txt para la siguiente iteracion\n",
    "    batch_index += 1\n",
    "    with open(index_file, \"w\") as f:\n",
    "        f.write(str(batch_index))\n",
    "    \n",
    "print(\"Se han procesado todos los archivos de la carpeta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaf8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# ELIMINADOR DE LOS .fits YA PROCESADOS\n",
    "\n",
    "pickle_folder = 'extra'\n",
    "spectrums_folder = r'spectrums'\n",
    "\n",
    "# Obtener la lista de archivos pickle que comienzan con \"batch_files_list\" y terminan con \".pkl\"\n",
    "pickle_files = [f for f in os.listdir(pickle_folder) if f.startswith('batch_files_list') and f.endswith('.pkl')]\n",
    "\n",
    "# Recorrer cada archivo pickle\n",
    "for pickle_file in pickle_files:\n",
    "    pickle_path = os.path.join(pickle_folder, pickle_file)\n",
    "    print(f\"Procesando {pickle_path}...\")\n",
    "    try:\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            file_list = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error abriendo {pickle_path}: {e}\")\n",
    "        continue\n",
    "    # Recorrer la lista de nombres y eliminar los archivos correspondientes en la carpeta spectrums\n",
    "    for filename in file_list:\n",
    "        file_path = os.path.join(spectrums_folder, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Eliminado: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error eliminando {file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado: {file_path}\")\n",
    "\n",
    "print(\"Proceso de eliminación completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# CONVERSION A NUMPY ARRAYS\n",
    "\n",
    "# Buscar todos los archivos que comiencen con un cierto prefijo en la carpeta \"data\"\n",
    "data_files = glob.glob(os.path.join('data', 'spectra_data_complete*.pkl'))\n",
    "data_files = sorted(data_files)\n",
    "print(f\"Se encontraron {len(data_files)} archivos pickle.\")\n",
    "\n",
    "# Contar el numero total de espectros\n",
    "total_spectra = 0\n",
    "for file in data_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        data_part = pickle.load(f)\n",
    "    total_spectra += len(data_part)\n",
    "print(f\"Total de espectros: {total_spectra}\")\n",
    "\n",
    "# Prealojar archivos memmap para flux, wavelength y redshift en la carpeta 'data'\n",
    "flux_mmap_path = os.path.join('data', 'spectra_data_complete_flux.dat')\n",
    "wavelength_mmap_path = os.path.join('data', 'spectra_data_complete_wavelength.dat')\n",
    "redshift_mmap_path = os.path.join('data', 'spectra_data_complete_redshift.dat')\n",
    "\n",
    "num_points = 5000\n",
    "flux_mmap = np.memmap(flux_mmap_path, dtype='float32', mode='w+', shape=(total_spectra, num_points))\n",
    "wavelength_mmap = np.memmap(wavelength_mmap_path, dtype='float32', mode='w+', shape=(total_spectra, num_points))\n",
    "redshift_mmap = np.memmap(redshift_mmap_path, dtype='float32', mode='w+', shape=(total_spectra,))\n",
    "\n",
    "# Cargar datos de los archivos pickle y escribirlos en los memmaps\n",
    "current_index = 0\n",
    "for file in data_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        data_part = pickle.load(f)\n",
    "    print(f\"Procesando {file} con {len(data_part)} espectros...\")\n",
    "    for key in data_part:\n",
    "        espectro = data_part[key]\n",
    "        flux = espectro[\"flux\"]              # Array de 5000 puntos\n",
    "        wavelength = espectro[\"wavelength\"]  # Array de 5000 puntos\n",
    "        redshift = espectro[\"redshift\"]      # Valor escalar\n",
    "        \n",
    "        # Almacenar en el memmap\n",
    "        flux_mmap[current_index, :] = flux\n",
    "        wavelength_mmap[current_index, :] = wavelength\n",
    "        redshift_mmap[current_index] = redshift\n",
    "        \n",
    "        current_index += 1\n",
    "\n",
    "# Asegurar que todos los cambios se escriban en disco\n",
    "flux_mmap.flush()\n",
    "wavelength_mmap.flush()\n",
    "redshift_mmap.flush()\n",
    "\n",
    "print(\"Se han guardado los datos en archivos memmap dentro de la carpeta 'data'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============\n",
    "# INTERPOLACION DE LOS ESPECTROS Y GUARDADO EN FORMATO PICKLE (VARIANTE PARA EL NED)\n",
    "\n",
    "folder_path = r'spectrums NASA'\n",
    "batch_size = 100000\n",
    "index_file = \"extra/batch_index.txt\"\n",
    "\n",
    "# Verificar existencia de batch_index.txt\n",
    "if not os.path.exists(index_file):\n",
    "    print(f\"Error: El archivo {index_file} no existe. Deteniendo la ejecución.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Leer y validar batch_index\n",
    "with open(index_file, \"r\") as f:\n",
    "    content = f.read().strip()\n",
    "    try:\n",
    "        batch_index = int(content)\n",
    "    except ValueError:\n",
    "        print(f\"Error: El archivo {index_file} no contiene un número entero válido. Deteniendo la ejecución.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def expand_points(wavelength, flux, target_count=5000):\n",
    "    wl = list(wavelength)\n",
    "    fl = list(flux)\n",
    "    # Diferencias absolutas entre puntos consecutivos\n",
    "    diffs = [abs(fl[i+1] - fl[i]) for i in range(len(fl)-1)]\n",
    "    sorted_indices = sorted(range(len(diffs)), key=lambda i: diffs[i], reverse=True)\n",
    "    # Insertar hasta alcanzar target_count\n",
    "    while len(wl) < target_count:\n",
    "        for idx in sorted_indices:\n",
    "            if len(wl) >= target_count:\n",
    "                break\n",
    "            new_wl = (wl[idx] + wl[idx+1]) / 2\n",
    "            new_fl = (fl[idx] + fl[idx+1]) / 2\n",
    "            wl.insert(idx+1, new_wl)\n",
    "            fl.insert(idx+1, new_fl)\n",
    "    return np.array(wl), np.array(fl)\n",
    "\n",
    "# Lista de archivos FITS\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.fits')]\n",
    "\n",
    "while files:\n",
    "    current_batch = files.copy() if len(files) < batch_size else random.sample(files, batch_size)\n",
    "    spectra_data = {}\n",
    "    processed = 0\n",
    "\n",
    "    for filename in current_batch:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with fits.open(file_path) as hdul:\n",
    "                try:\n",
    "                    data    = hdul[0].data      # shape (5,3856)\n",
    "                    header0 = hdul[0].header\n",
    "\n",
    "                    # Construir vector de longitudes de onda (log‑linear)\n",
    "                    n_pix  = header0['NAXIS1']\n",
    "                    coeff0 = header0['COEFF0']\n",
    "                    coeff1 = header0['COEFF1']\n",
    "                    crpix1 = header0.get('CRPIX1', 1)\n",
    "                    pix    = np.arange(n_pix)\n",
    "                    wave   = 10**(coeff0 + coeff1 * (pix + 1 - crpix1))\n",
    "                    flux  = data[1, :]\n",
    "                    redshift   = header0.get('Z')\n",
    "\n",
    "                    # Limpiar NaN antes de expandir\n",
    "                    mask = np.isfinite(flux)\n",
    "                    wavelength_data = wave[mask]\n",
    "                    flux_data = flux[mask]\n",
    "\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "                # Interpolar hasta 5000 puntos en flux\n",
    "                interp_wavelength = wavelength_data\n",
    "                interp_flux = flux_data\n",
    "                # Aplicar expand_points en bucle\n",
    "                while interp_flux.shape[0] < 5000:\n",
    "                    interp_wavelength, interp_flux = expand_points(interp_wavelength, interp_flux, target_count=5000)\n",
    "\n",
    "                spectra_data[filename] = {\n",
    "                    \"wavelength\": interp_wavelength,\n",
    "                    \"flux\": interp_flux,\n",
    "                    \"redshift\": redshift\n",
    "                }\n",
    "                processed += 1\n",
    "                if processed % 1000 == 0:\n",
    "                    print(f\"Procesado {processed} espectros en este lote.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {filename}: {e}\")  # sigue con el siguiente archivo\n",
    "\n",
    "    print(f\"Se procesaron {len(spectra_data)} archivos FITS en el lote {batch_index}.\")\n",
    "\n",
    "    # Guardar resultados\n",
    "    output_file = f'data/spectra_data_NASA{batch_index}.pkl'\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(spectra_data, f)\n",
    "    print(f\"Datos guardados en {output_file}\")\n",
    "\n",
    "    batch_list_file = f'extra/batch_files_list{batch_index}.pkl'\n",
    "    with open(batch_list_file, 'wb') as f:\n",
    "        pickle.dump(current_batch, f)\n",
    "    print(f\"Lista de archivos del lote {batch_index} guardada en {batch_list_file}\")\n",
    "\n",
    "    # Actualizar lista y batch_index\n",
    "    files = [f for f in files if f not in current_batch]\n",
    "    batch_index += 1\n",
    "    with open(index_file, \"w\") as f:\n",
    "        f.write(str(batch_index))\n",
    "\n",
    "print(\"Se han procesado todos los archivos de la carpeta.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TFGF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
